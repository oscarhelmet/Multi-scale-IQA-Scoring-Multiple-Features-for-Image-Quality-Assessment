{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734cee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c47bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Singlescale data\n",
    "\n",
    "dataset = pd.read_csv(\"Dataset/Training_Set/data1.csv\", header=None)\n",
    "\n",
    "scaler = StandardScaler().fit(dataset)\n",
    "dataset_scaled = scaler.transform(dataset)\n",
    "\n",
    "labels = pd.read_csv(\"Dataset/Training_Set/label.csv\", header=None)\n",
    "\n",
    "labels = labels.astype(int).values.squeeze()-1\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "labels_one_hot = one_hot_encode(labels, num_classes)\n",
    "\n",
    "dataset_tensor = torch.tensor(dataset_scaled, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_one_hot, dtype=torch.float32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dataset_tensor = dataset_tensor.to(device)\n",
    "labels_tensor = labels_tensor.to(device)\n",
    "\n",
    "print(labels_tensor.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2735368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singlescale(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Singlescale, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(64, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.layer4 = nn.Linear(64, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 64)\n",
    "        self.layer7 = nn.Linear(64, 10)\n",
    "\n",
    "       \n",
    "        self.dropout = nn.Dropout(p=0.85)\n",
    "        \n",
    "        init.kaiming_normal_(self.layer1.weight)\n",
    "        init.kaiming_normal_(self.layer2.weight)\n",
    "        init.kaiming_normal_(self.layer3.weight)\n",
    "        init.kaiming_normal_(self.layer4.weight)\n",
    "        init.kaiming_normal_(self.layer5.weight)\n",
    "        init.kaiming_normal_(self.layer6.weight)\n",
    "        init.kaiming_normal_(self.layer7.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))        \n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer4(x))        \n",
    "        x = F.relu(self.layer5(x))        \n",
    "        x = F.relu(self.layer6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer7(x) \n",
    "        return F.log_softmax(x, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28833be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiscale(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Multiscale, self).__init__()\n",
    "        self.layer1 = nn.Linear(1472, 1472)\n",
    "        self.layer2 = nn.Linear(1472, 1472)\n",
    "        self.layer3 = nn.Linear(1472, 1472)\n",
    "        self.layer4 = nn.Linear(1472, 1472)\n",
    "        self.layer5 = nn.Linear(1472, 1472) \n",
    "        self.layer6 = nn.Linear(1472, 1472) \n",
    "        self.layer7 = nn.Linear(1472, 10)\n",
    "\n",
    "       \n",
    "        self.dropout = nn.Dropout(p=0.85)\n",
    "        \n",
    "        init.kaiming_normal_(self.layer1.weight)\n",
    "        init.kaiming_normal_(self.layer2.weight)\n",
    "        init.kaiming_normal_(self.layer3.weight)\n",
    "        init.kaiming_normal_(self.layer4.weight)\n",
    "        init.kaiming_normal_(self.layer5.weight)\n",
    "        init.kaiming_normal_(self.layer6.weight)\n",
    "        init.kaiming_normal_(self.layer7.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))       \n",
    "        x = F.relu(self.layer2(x))        \n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        x = F.relu(self.layer6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer10(x) \n",
    "        return F.log_softmax(x, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def fit(num_epochs, model, loss_fn, optimizer, train_dataloader, val_loader):\n",
    "    train_losses = []  # Stores training losses for each epoch\n",
    "    val_losses = []  # Stores validation losses for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Ensure the model is in training mode\n",
    "        epoch_training_loss = 0  # For accumulating the loss for this epoch\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "            preds = model(inputs)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            epoch_training_loss += loss.item()  # Add the loss for this batch\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Record the average training loss for this epoch\n",
    "        train_losses.append(epoch_training_loss / len(train_dataloader))\n",
    "\n",
    "        model.eval()  # Switch the model to evaluation mode\n",
    "        epoch_val_loss = 0  # For accumulating the loss for this epoch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_targets in val_loader:\n",
    "                val_input, val_targets = val_input.float().to(device), val_targets.float().to(device)\n",
    "                out = model(val_input)\n",
    "                val_loss = loss_fn(out, val_targets)\n",
    "                epoch_val_loss += val_loss.item()  # Add the loss for this batch\n",
    "\n",
    "        # Record the average validation loss for this epoch\n",
    "        val_losses.append(epoch_val_loss / len(val_loader))\n",
    "\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch [{}/{}], Training loss: {:.4f}, Validation Loss: {:.4f}\"\n",
    "              .format(epoch + 1, num_epochs, train_losses[-1], val_losses[-1]))\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        def evaluate_model(model, val_dataloader, loss_fn, device):\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():  # Do not calculate gradients since we are only evaluating\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "                    # Find the predicted labels by taking the argmax of the outputs\n",
    "                    _, pred = torch.max(outputs, 1)\n",
    "                    _, labels = torch.max(targets, 1)\n",
    "                    correct += (pred == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            accuracy = correct / len(val_dataloader.dataset)\n",
    "\n",
    "            return val_loss, accuracy\n",
    "\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_dataloader, loss_fn, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses  # Return the losses for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932577cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input hyperparameter \n",
    "num_epochs = 800\n",
    "batch_size = 128\n",
    "learning_rate= 0.001\n",
    "\n",
    "model = Singlescale()\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr =learning_rate)\n",
    "print(dataset_tensor.shape)\n",
    "print(labels_tensor.shape)\n",
    "\n",
    "\n",
    "# # Making sure shapes are correct and defining dataloader.\n",
    "# input_np_array = np.array(dataset[:,1:-1].tolist(), dtype='float32')\n",
    "# inputs = torch.from_numpy(input_np_array)\n",
    "# # inputs = torch.from_numpy(np.array(dataset.iloc[:,1:-1].values, dtype='float32'))\n",
    "# print(inputs.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_tensor_dataset = TensorDataset(dataset_tensor, labels_tensor)\n",
    "val_size = int(0.22 * len(train_tensor_dataset))  # 22% for validation\n",
    "train_size = len(train_tensor_dataset) - val_size  # 78% for training\n",
    "train_data, val_data = random_split(train_tensor_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size)\n",
    "\n",
    "# # Train the model.\n",
    "train_losses,val_losses = fit(num_epochs, model, loss_fn, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "test_data = pd.read_csv('Dataset/Test_set/test1.csv', header=None)\n",
    "scaler = StandardScaler().fit(test_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "test_labels = pd.read_csv('Dataset/Test_Set/label.csv', header=None)\n",
    "\n",
    "test_labels = test_labels.astype(int).values.squeeze() - 1\n",
    "num_classes = test_labels.max() + 1\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "test_labels_one_hot = one_hot_encode(test_labels, num_classes)\n",
    "\n",
    "test_data = torch.tensor(test_scaled, dtype=torch.float32).to(device)\n",
    "test_labels = torch.tensor(test_labels_one_hot, dtype=torch.float32).to(device)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "test_dataset = TestDataset(test_data, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "def plcc(x, y):\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x)\n",
    "    if isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    if len(x.shape) != 1 or len(y.shape) != 1:\n",
    "        raise Exception(\"Please input N (* 1) vector.\")\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise Exception(\"The lengths of 2 input vectors are not equal.\")\n",
    "\n",
    "    x = x - np.average(x)\n",
    "    y = y - np.average(y)\n",
    "    numerator = np.dot(x, y)\n",
    "    denominator = np.sqrt(np.sum(x ** 2)) * np.sqrt(np.sum(y ** 2))\n",
    "    return numerator / denominator\n",
    "\n",
    "def srocc(x, y):\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x)\n",
    "    if isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    if len(x.shape) != 1 or len(y.shape) != 1:\n",
    "        raise Exception(\"Please input N (* 1) vector.\")\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise Exception(\"The lengths of 2 input vectors are not equal.\")\n",
    "\n",
    "    rank_x = x.argsort().argsort()\n",
    "    rank_y = y.argsort().argsort()\n",
    "    return plcc(rank_x, rank_y)\n",
    "\n",
    "model = Singlescale().to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "pred_scores = []\n",
    "true_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        test_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(targets, 1)\n",
    "\n",
    "        pred_scores.extend(pred.cpu().numpy())\n",
    "        true_scores.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "plcc_score = plcc(true_scores, pred_scores)\n",
    "srocc_score = srocc(true_scores, pred_scores)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"PLCC: {plcc_score:.4f}\")\n",
    "print(f\"SROCC: {srocc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_single_{learning_rate}_{batch_size}_{num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input hyperparameter \n",
    "num_epochs = 800\n",
    "batch_size = 128\n",
    "learning_rate= 0.001\n",
    "\n",
    "model = Multiscale()\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr =learning_rate)\n",
    "print(dataset_tensor.shape)\n",
    "print(labels_tensor.shape)\n",
    "\n",
    "\n",
    "train_tensor_dataset = TensorDataset(dataset_tensor, labels_tensor)\n",
    "val_size = int(0.22 * len(train_tensor_dataset))  # 22% for validation\n",
    "train_size = len(train_tensor_dataset) - val_size  # 78% for training\n",
    "train_data, val_data = random_split(train_tensor_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size)\n",
    "\n",
    "# # Train the model.\n",
    "train_losses,val_losses = fit(num_epochs, model, loss_fn, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac10445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_data = pd.read_csv('Dataset/Test_set/test.csv', header=None)\n",
    "scaler = StandardScaler().fit(test_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "test_labels = pd.read_csv('Dataset/Test_Set/label.csv', header=None)\n",
    "\n",
    "test_labels = test_labels.astype(int).values.squeeze() - 1\n",
    "num_classes = test_labels.max() + 1\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "test_labels_one_hot = one_hot_encode(test_labels, num_classes)\n",
    "\n",
    "test_data = torch.tensor(test_scaled, dtype=torch.float32).to(device)\n",
    "test_labels = torch.tensor(test_labels_one_hot, dtype=torch.float32).to(device)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "test_dataset = TestDataset(test_data, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "def plcc(x, y):\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x)\n",
    "    if isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    if len(x.shape) != 1 or len(y.shape) != 1:\n",
    "        raise Exception(\"Please input N (* 1) vector.\")\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise Exception(\"The lengths of 2 input vectors are not equal.\")\n",
    "\n",
    "    x = x - np.average(x)\n",
    "    y = y - np.average(y)\n",
    "    numerator = np.dot(x, y)\n",
    "    denominator = np.sqrt(np.sum(x ** 2)) * np.sqrt(np.sum(y ** 2))\n",
    "    return numerator / denominator\n",
    "\n",
    "def srocc(x, y):\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x)\n",
    "    if isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    if len(x.shape) != 1 or len(y.shape) != 1:\n",
    "        raise Exception(\"Please input N (* 1) vector.\")\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise Exception(\"The lengths of 2 input vectors are not equal.\")\n",
    "\n",
    "    rank_x = x.argsort().argsort()\n",
    "    rank_y = y.argsort().argsort()\n",
    "    return plcc(rank_x, rank_y)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "pred_scores = []\n",
    "true_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        test_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(targets, 1)\n",
    "\n",
    "        pred_scores.extend(pred.cpu().numpy())\n",
    "        true_scores.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "plcc_score = plcc(true_scores, pred_scores)\n",
    "srocc_score = srocc(true_scores, pred_scores)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"PLCC: {plcc_score:.4f}\")\n",
    "print(f\"SROCC: {srocc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_multi_{learning_rate}_{batch_size}_{num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b369c0b-16eb-4c94-a12c-9d3bc7ff490e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
